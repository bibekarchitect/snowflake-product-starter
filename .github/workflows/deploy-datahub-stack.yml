name: Deploy DataHub Stack (CloudSQL + ES + DataHub)

concurrency:
  group: datahub-deploy
  cancel-in-progress: false

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "tfvars env & K8s target (maps to infra/cloudsql/envs/<env>.tfvars)"
        type: choice
        options: [dev]
        default: dev
      plan_only:
        description: "Plan/Preview only (no apply; skip Helm installs)"
        type: boolean
        default: false

jobs:
  deploy:
    runs-on: ubuntu-latest

    env:
      # Paths (adjust if you changed repo layout)
      NS: datahub
      SQL_DIR: infra/cloudsql
      ES_VALUES: infra/elastic/values.elasticsearch.yaml
      DH_VALUES: infra/datahub/values/values.prod.yaml
      TFVARS: infra/cloudsql/envs/${{ github.event.inputs.environment }}.tfvars

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ---------- Google Auth & GKE ----------
      - name: Google auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
          project_id:       ${{ secrets.GCP_PROJECT_ID }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2

      # Validate GKE env first and export to $GITHUB_ENV to avoid "input required" errors
      - name: Load & validate GKE parameters
        shell: bash
        env:
          GKE_CLUSTER_NAME: ${{ secrets.GKE_CLUSTER_NAME }}
          GKE_LOCATION:     ${{ secrets.GKE_LOCATION }}
        run: |
          echo "::group::Validate GKE inputs"
          : "${GKE_CLUSTER_NAME:?GKE_CLUSTER_NAME is missing (Settings→Secrets→Actions)}"
          : "${GKE_LOCATION:?GKE_LOCATION is missing (Settings→Secrets→Actions)}"
          echo "Cluster/Location present ✅"
          echo "::endgroup::"
          echo "GKE_CLUSTER_NAME=$GKE_CLUSTER_NAME" >> "$GITHUB_ENV"
          echo "GKE_LOCATION=$GKE_LOCATION"         >> "$GITHUB_ENV"

      - name: Get GKE credentials
        uses: google-github-actions/get-gke-credentials@v2
        with:
          cluster_name: ${{ env.GKE_CLUSTER_NAME }}
          location:     ${{ env.GKE_LOCATION }}
          # If your cluster lives in a different project:
          # project_id:   ${{ secrets.GCP_PROJECT_ID }}

      # ---------- Terraform (Cloud SQL) ----------
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.7.5"

      - name: Terraform init (Cloud SQL)
        working-directory: ${{ env.SQL_DIR }}
        run: terraform init -upgrade -reconfigure

      - name: Terraform plan (Cloud SQL)
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        working-directory: ${{ env.SQL_DIR }}
        run: terraform plan -no-color -var-file="envs/dev.tfvars"

      - name: Terraform apply (Cloud SQL)
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        working-directory: ${{ env.SQL_DIR }}
        run: terraform apply -auto-approve -no-color -var-file="envs/dev.tfvars"

      # Install jq to parse TF outputs robustly
      - name: Install jq
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        run: sudo apt-get update && sudo apt-get install -y jq

      # Only capture outputs if apply actually ran
      - name: Capture Cloud SQL outputs
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        id: sqlout
        working-directory: ${{ env.SQL_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          terraform output -json > /tmp/tfout.json
          conn=$(jq -r '.instance_connection_name.value // empty' /tmp/tfout.json)
          db=$(jq -r '.db_name.value // empty' /tmp/tfout.json)
          user=$(jq -r '.db_user.value // empty' /tmp/tfout.json)

          if [ -z "$conn" ]; then
            echo "⚠️ No instance_connection_name output found; ensure outputs.tf defines it."
          fi

          echo "conn=$conn"  >> "$GITHUB_OUTPUT"
          echo "db=$db"      >> "$GITHUB_OUTPUT"
          echo "user=$user"  >> "$GITHUB_OUTPUT"

      # ---------- Kubernetes namespace & Helm ----------
      - name: Ensure namespace
        run: kubectl create ns $NS --dry-run=client -o yaml | kubectl apply -f -

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Add Helm repos
        run: |
          helm repo add elastic https://helm.elastic.co
          helm repo add datahub https://helm.datahubproject.io
          helm repo update

      # (Optional) Helm diffs can be added if you install "helm diff" plugin.
      # Skipped here for simplicity to avoid plugin dependency.

      # ---------- Elasticsearch on GKE ----------
      - name: Install/upgrade Elasticsearch
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          helm upgrade -i elasticsearch elastic/elasticsearch -n $NS -f $ES_VALUES
          kubectl -n $NS rollout status statefulset/elasticsearch-master --timeout=10m || true

      # ---------- K8s secrets for DataHub ----------
      - name: Kafka Secret (Confluent)
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          kubectl -n $NS delete secret confluent-kafka-secret --ignore-not-found
          kubectl -n $NS create secret generic confluent-kafka-secret \
            --from-literal=sasl_jaas_config='org.apache.kafka.common.security.plain.PlainLoginModule required username="${{ secrets.CONFLUENT_API_KEY }}" password="${{ secrets.CONFLUENT_API_SECRET }}";'

      - name: DB Secret (Cloud SQL)
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') && steps.sqlout.outputs.conn != '' }}
        run: |
          kubectl -n $NS delete secret datahub-db-secret --ignore-not-found
          kubectl -n $NS create secret generic datahub-db-secret \
            --from-literal=instance_connection_name='${{ steps.sqlout.outputs.conn }}' \
            --from-literal=db_password='${{ secrets.CLOUDSQL_DB_PASS }}'

      # ---------- DataHub (GMS + Frontend) ----------
      - name: Install/upgrade DataHub
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          helm upgrade -i datahub datahub/datahub -n $NS -f $DH_VALUES

      - name: Wait for DataHub pods
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          kubectl -n $NS rollout status deploy/datahub-gms --timeout=10m
          kubectl -n $NS rollout status deploy/datahub-frontend --timeout=10m

      - name: Show pods
        run: kubectl -n $NS get pods -o wide
      - name: Debug plan_only
        run: |
          echo "raw='${{ github.event.inputs.plan_only }}'"
          echo "as-bool='${{ fromJSON(github.event.inputs.plan_only || 'false') }}'"
      
      - name: Wait for Elasticsearch
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          kubectl -n datahub rollout status statefulset/elasticsearch-master --timeout=10m

      - name: Reset ES setup job
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          kubectl -n datahub delete job datahub-elasticsearch-setup-job --ignore-not-found
          
      - name: Hard reset Helm release "datahub"
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
            set -e
            NS=datahub
            REL=datahub
            # Uninstall if exists (ignore errors)
            helm -n "$NS" uninstall "$REL" || true
            # Remove any leftover Helm release records (secrets/configmaps)
            kubectl -n "$NS" delete secret    -l "owner=helm,name=$REL" --ignore-not-found
            kubectl -n "$NS" delete configmap -l "owner=helm,name=$REL" --ignore-not-found

      - name: Upgrade DataHub (pick up ES auth)
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          helm upgrade datahub datahub/datahub -n datahub \
            -f infra/datahub/values/values.prod.yaml

      - name: Clear pending Helm release (if any)
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
            set -euo pipefail
            if helm -n datahub status datahub 2>/dev/null | grep -qi 'pending'; then
            echo "Helm release is pending; trying rollback…"
            LAST_OK=$(helm -n datahub history datahub --output json | jq -r '[.[]|select(.status=="deployed")][-1].revision // empty')
            if [ -n "$LAST_OK" ]; then
                helm -n datahub rollback datahub "$LAST_OK" --cleanup-on-fail
            else
                echo "No deployed revision; uninstalling release…"
                helm -n datahub uninstall datahub || true
            fi
            fi
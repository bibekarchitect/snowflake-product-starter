name: Deploy DataHub Stack (CloudSQL + ES + DataHub)

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "tfvars env & K8s target (maps to infra/cloudsql/envs/<env>.tfvars)"
        type: choice
        options: [dev]
        default: dev
      plan_only:
        description: "Plan/Preview only (no apply; skip Helm installs)"
        type: boolean
        default: false

jobs:
  deploy:
    runs-on: ubuntu-latest

    env:
      # Paths (adjust if you changed repo layout)
      NS: datahub
      SQL_DIR: infra/cloudsql
      ES_VALUES: infra/elastic/values.elasticsearch.yaml
      DH_VALUES: infra/datahub/values/values.prod.yaml
      TFVARS: infra/cloudsql/envs/${{ github.event.inputs.environment }}.tfvars

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ---------- Google Auth & GKE ----------
      - name: Google auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
          project_id:       ${{ secrets.GCP_PROJECT_ID }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2

      # Validate GKE env first and export to $GITHUB_ENV to avoid "input required" errors
      - name: Load & validate GKE parameters
        shell: bash
        env:
          GKE_CLUSTER_NAME: ${{ secrets.GKE_CLUSTER_NAME }}
          GKE_LOCATION:     ${{ secrets.GKE_LOCATION }}
        run: |
          echo "::group::Validate GKE inputs"
          : "${GKE_CLUSTER_NAME:?GKE_CLUSTER_NAME is missing (Settings→Secrets→Actions)}"
          : "${GKE_LOCATION:?GKE_LOCATION is missing (Settings→Secrets→Actions)}"
          echo "Cluster/Location present ✅"
          echo "::endgroup::"
          echo "GKE_CLUSTER_NAME=$GKE_CLUSTER_NAME" >> "$GITHUB_ENV"
          echo "GKE_LOCATION=$GKE_LOCATION"         >> "$GITHUB_ENV"

      - name: Get GKE credentials
        uses: google-github-actions/get-gke-credentials@v2
        with:
          cluster_name: ${{ env.GKE_CLUSTER_NAME }}
          location:     ${{ env.GKE_LOCATION }}
          # If your cluster lives in a different project:
          # project_id:   ${{ secrets.GCP_PROJECT_ID }}

      # ---------- Terraform (Cloud SQL) ----------
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.7.5"

      - name: Terraform init (Cloud SQL)
        working-directory: ${{ env.SQL_DIR }}
        run: terraform init -upgrade -reconfigure

      - name: Terraform plan (Cloud SQL)
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        working-directory: ${{ env.SQL_DIR }}
        run: terraform plan -no-color -var-file="envs/dev.tfvars"

      - name: Terraform apply (Cloud SQL)
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        working-directory: ${{ env.SQL_DIR }}
        run: terraform apply -auto-approve -no-color -var-file="${{ env.TFVARS }}"

      # Install jq to parse TF outputs robustly
      - name: Install jq
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        run: sudo apt-get update && sudo apt-get install -y jq

      # Only capture outputs if apply actually ran
      - name: Capture Cloud SQL outputs
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        id: sqlout
        working-directory: ${{ env.SQL_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          terraform output -json > /tmp/tfout.json
          conn=$(jq -r '.instance_connection_name.value // empty' /tmp/tfout.json)
          db=$(jq -r '.db_name.value // empty' /tmp/tfout.json)
          user=$(jq -r '.db_user.value // empty' /tmp/tfout.json)

          if [ -z "$conn" ]; then
            echo "⚠️ No instance_connection_name output found; ensure outputs.tf defines it."
          fi

          echo "conn=$conn"  >> "$GITHUB_OUTPUT"
          echo "db=$db"      >> "$GITHUB_OUTPUT"
          echo "user=$user"  >> "$GITHUB_OUTPUT"

      # ---------- Kubernetes namespace & Helm ----------
      - name: Ensure namespace
        run: kubectl create ns $NS --dry-run=client -o yaml | kubectl apply -f -

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Add Helm repos
        run: |
          helm repo add elastic https://helm.elastic.co
          helm repo add datahub https://helm.datahubproject.io
          helm repo update

      # (Optional) Helm diffs can be added if you install "helm diff" plugin.
      # Skipped here for simplicity to avoid plugin dependency.

      # ---------- Elasticsearch on GKE ----------
      - name: Install/upgrade Elasticsearch
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          helm upgrade -i elasticsearch elastic/elasticsearch -n $NS -f $ES_VALUES
          kubectl -n $NS rollout status statefulset/elasticsearch-master --timeout=10m || true

      # ---------- K8s secrets for DataHub ----------
      - name: Kafka Secret (Confluent)
        if: ${{ fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          kubectl -n $NS delete secret confluent-kafka-secret --ignore-not-found
          kubectl -n $NS create secret generic confluent-kafka-secret \
            --from-literal=sasl_jaas_config='org.apache.kafka.common.security.plain.PlainLoginModule required username="${{ secrets.CONFLUENT_API_KEY }}" password="${{ secrets.CONFLUENT_API_SECRET }}";'

      - name: DB Secret (Cloud SQL)
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') && steps.sqlout.outputs.conn != '' }}
        run: |
          kubectl -n $NS delete secret datahub-db-secret --ignore-not-found
          kubectl -n $NS create secret generic datahub-db-secret \
            --from-literal=instance_connection_name='${{ steps.sqlout.outputs.conn }}' \
            --from-literal=db_password='${{ secrets.CLOUDSQL_DB_PASS }}'

      # ---------- DataHub (GMS + Frontend) ----------
      - name: Install/upgrade DataHub
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          helm upgrade -i datahub datahub/datahub -n $NS -f $DH_VALUES

      - name: Wait for DataHub pods
        if: ${{ !fromJSON(github.event.inputs.plan_only || 'false') }}
        run: |
          kubectl -n $NS rollout status deploy/datahub-gms --timeout=10m
          kubectl -n $NS rollout status deploy/datahub-frontend --timeout=10m

      - name: Show pods
        run: kubectl -n $NS get pods -o wide
      - name: Debug plan_only
        run: |
          echo "raw='${{ github.event.inputs.plan_only }}'"
          echo "as-bool='${{ fromJSON(github.event.inputs.plan_only || 'false') }}'"